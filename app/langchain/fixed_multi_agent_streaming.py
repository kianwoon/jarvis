"""
Fixed multi-agent streaming with thinking, tools, and collaboration
Enhanced with comprehensive Langfuse tracing
"""
import json
import asyncio
import time
import re
from concurrent.futures import ThreadPoolExecutor
from typing import Optional
from app.langchain.langgraph_multi_agent_system import LangGraphMultiAgentSystem
from app.llm.ollama import OllamaLLM
from app.llm.base import LLMConfig
from app.core.mcp_tools_cache import get_enabled_mcp_tools
from app.core.langfuse_integration import get_tracer
from app.core.llm_settings_cache import get_llm_settings, get_second_llm_full_config
from app.core.intelligent_agent_selector import IntelligentAgentSelector
from app.core.agent_performance_tracker import performance_tracker
from app.core.langgraph_agents_cache import get_agent_by_name
# from app.langchain.tool_executor import tool_executor  # Not needed for current implementation
from app.langchain.service import call_mcp_tool
import logging

logger = logging.getLogger(__name__)

async def fixed_multi_agent_streaming(
    question: str,
    conversation_id: Optional[str] = None,
    trace=None
):
    """
    Fixed multi-agent streaming addressing all three issues:
    1. Tool usage from agent configs  
    2. True agent collaboration
    3. Comprehensive Langfuse tracing
    """
    
    async def stream_fixed_multi_agent():
        # Initialize Langfuse tracing
        tracer = get_tracer()
        main_trace = trace  # Use provided trace or create new one
        workflow_span = None
        
        # Initialize system with LLM settings
        get_llm_settings()  # Ensure settings are loaded
        
        try:
            yield json.dumps({"type": "status", "message": "üéØ Starting advanced multi-agent collaboration..."}) + "\n"
            await asyncio.sleep(0.3)
            
            # Initialize system
            system = LangGraphMultiAgentSystem(conversation_id)
            
            # Create multi-agent workflow span if tracing is enabled
            if main_trace and tracer.is_enabled():
                try:
                    workflow_span = tracer.create_multi_agent_workflow_span(
                        main_trace, "fixed_multi_agent_streaming", []
                    )
                    logger.info("Created multi-agent workflow span for fixed streaming")
                except Exception as e:
                    logger.warning(f"Failed to create workflow span: {e}")
            
            # Get agents from database and available tools
            available_agents = system.agents
            available_tools = get_enabled_mcp_tools()
            
            if not available_agents:
                yield json.dumps({"type": "error", "error": "No agents found in database"}) + "\n"
                return
            
            # Intelligent agent selection using semantic analysis
            try:
                agent_selector = IntelligentAgentSelector()
                selected_agents, question_analysis = agent_selector.select_agents(question, available_tools)
            except Exception as e:
                logger.error(f"Agent selection failed: {e}")
                # Create default analysis for fallback
                from app.core.intelligent_agent_selector import QuestionAnalysis
                question_analysis = QuestionAnalysis(
                    complexity="moderate", domain="general", required_skills=[], 
                    tool_requirements=[], collaboration_type="sequential", 
                    optimal_agent_count=2, keywords=[], confidence=0.5
                )
                selected_agents = []
            
            # Fallback to basic selection if intelligent selector fails
            if not selected_agents:
                logger.warning("Intelligent agent selection failed, using capability-based fallback")
                # Use capability-based fallback instead of arbitrary selection
                from app.core.langgraph_agents_cache import get_agents_with_capabilities
                agents_with_caps = get_agents_with_capabilities()
                if agents_with_caps:
                    # Select diverse agents based on domain
                    domain_agents = {}
                    for name, data in agents_with_caps.items():
                        domain = data.get('capabilities', {}).get('primary_domain', 'general')
                        if domain not in domain_agents:
                            domain_agents[domain] = name
                    selected_agents = list(domain_agents.values())[:3]
                else:
                    # Last resort
                    selected_agents = list(available_agents.keys())[:3]
                logger.info(f"Fallback selected: {selected_agents}")
                
            agents_used = selected_agents
            
            # Log selection details
            logger.info(f"üéØ ORIGINAL SELECTION: {selected_agents} (count: {len(selected_agents)})")
            logger.info(f"üéØ AGENTS TO PROCESS: {agents_used} (count: {len(agents_used)})")
            logger.info(f"Question analysis: complexity={question_analysis.complexity}, "
                       f"domain={question_analysis.domain}, optimal_count={question_analysis.optimal_agent_count}, confidence={question_analysis.confidence:.2f}")
            
            # Yield selection details for transparency
            yield json.dumps({
                "type": "agent_selection",
                "selected_agents": agents_used,
                "selection_analysis": {
                    "complexity": question_analysis.complexity,
                    "domain": question_analysis.domain,
                    "collaboration_type": question_analysis.collaboration_type,
                    "optimal_count": question_analysis.optimal_agent_count,
                    "confidence": question_analysis.confidence
                },
                "message": f"üéØ Selected {len(agents_used)} agents for {question_analysis.complexity} {question_analysis.domain} task"
            }) + "\n"
            await asyncio.sleep(0.2)
            
            # Create agent selection span for tracing
            selection_span = None
            if workflow_span and tracer.is_enabled():
                try:
                    selection_span = tracer.create_span(
                        workflow_span,
                        name="agent-selection",
                        metadata={
                            "operation": "intelligent_agent_selection",
                            "selected_agents": agents_used,
                            "total_available": len(available_agents),
                            "selection_criteria": "semantic_analysis_with_capability_matching",
                            "question_complexity": question_analysis.complexity,
                            "question_domain": question_analysis.domain,
                            "collaboration_type": question_analysis.collaboration_type,
                            "selection_confidence": question_analysis.confidence,
                            "optimal_agent_count": question_analysis.optimal_agent_count
                        }
                    )
                    logger.info(f"Created agent selection span for agents: {agents_used}")
                except Exception as e:
                    logger.warning(f"Failed to create agent selection span: {e}")
            
            # Build detailed agent information
            agent_details = []
            for agent_name in agents_used:
                agent_data = available_agents[agent_name]
                agent_details.append({
                    "name": agent_name,
                    "role": agent_data.get('role', agent_name),
                    "description": agent_data.get('description', f"Specialized {agent_data.get('role', 'analysis')} expert"),
                    "tools": agent_data.get('tools', []),
                    "capabilities": agent_data.get('capabilities', []),
                    "estimated_time": f"{2 + len(agent_data.get('tools', []))} min"
                })
            
            yield json.dumps({
                "type": "agents_selected",
                "agents": agents_used,
                "agent_details": agent_details,
                "execution_plan": {
                    "total_agents": len(agents_used),
                    "estimated_total_time": f"{len(agents_used) * 3 + 2} min",
                    "execution_pattern": "sequential_collaborative",
                    "includes_synthesis": True
                },
                "selection_criteria": "tool_based_and_keyword_matching",
                "message": f"üéØ Selected {len(agents_used)} agents for collaborative analysis"
            }) + "\n"
            await asyncio.sleep(0.2)
            
            # End agent selection span
            if selection_span and tracer.is_enabled():
                try:
                    tracer.end_span_with_result(
                        selection_span,
                        {
                            "selected_agents": agents_used,
                            "selection_success": True,
                            "agent_count": len(agents_used)
                        },
                        success=True
                    )
                except Exception as e:
                    logger.warning(f"Failed to end agent selection span: {e}")
            
            # Track agent interactions
            agent_responses = {}
            agent_communications = []
            
            # Process each agent with collaboration
            # CRITICAL DEBUG: Log exact processing plan
            logger.info(f"üî• STARTING AGENT PROCESSING LOOP")
            logger.info(f"üî• Total agents to process: {len(agents_used)}")
            logger.info(f"üî• Agent list: {agents_used}")
            
            for i, agent_name in enumerate(agents_used):
                logger.info(f"üî• PROCESSING AGENT {i+1}/{len(agents_used)}: {agent_name}")
                
                if agent_name not in available_agents:
                    logger.error(f"üî• CRITICAL ERROR: Agent '{agent_name}' not found in available_agents!")
                    logger.error(f"üî• Available agents: {list(available_agents.keys())}")
                    continue
                    
                agent_data = available_agents[agent_name]
                role = agent_data.get('role', agent_name)
                tools = agent_data.get('tools', [])
                
                yield json.dumps({
                    "type": "agent_start",
                    "agent": agent_name,
                    "agent_info": {
                        "role": role,
                        "position": i + 1,
                        "total_agents": len(agents_used),
                        "tools": tools,
                        "phase": "individual_analysis"
                    },
                    "progress": {
                        "current_step": i + 1,
                        "total_steps": len(agents_used) + 1,  # +1 for synthesis
                        "percentage": int((i / (len(agents_used) + 1)) * 100),
                        "phase_name": f"Agent {i + 1} Analysis",
                        "next_step": "synthesis" if i == len(agents_used) - 1 else f"Agent {i + 2} ({available_agents[agents_used[i + 1]].get('role', agents_used[i + 1])}) analysis",
                        "estimated_remaining_time": f"{(len(agents_used) - i) * 2} min"
                    },
                    "message": f"üéØ {agent_name} ({role}) beginning analysis... ({i + 1}/{len(agents_used)})"
                }) + "\n"
                await asyncio.sleep(0.3)
                
                # Create agent execution span
                agent_span = None
                agent_generation = None
                if workflow_span and tracer.is_enabled():
                    try:
                        agent_span = tracer.create_agent_execution_span(
                            workflow_span,
                            agent_name,
                            question,
                            {
                                "agent_position": i + 1,
                                "total_agents": len(agents_used),
                                "execution_pattern": "collaborative_streaming"
                            }
                        )
                        logger.info(f"Created agent execution span for {agent_name}")
                    except Exception as e:
                        logger.warning(f"Failed to create agent execution span for {agent_name}: {e}")
                
                try:
                    # Start timing for performance tracking
                    agent_start_time = time.time()
                    
                    # Get agent config from database
                    agent_data = available_agents[agent_name]
                    system_prompt = agent_data.get('system_prompt', '')
                    role = agent_data.get('role', agent_name)
                    config = agent_data.get('config', {})
                    agent_tools = agent_data.get('tools', [])
                    
                    # Log agent configuration details 
                    logger.debug(f"üîç AGENT DEBUG {agent_name}:")
                    logger.debug(f"  - Config: {config}")
                    logger.debug(f"  - Has model in config: {'model' in config}")
                    logger.debug(f"  - Model value: {config.get('model', 'NOT SET')}")
                    logger.debug(f"  - Full agent data keys: {list(agent_data.keys())}")
                    
                    # Build collaboration context
                    collaboration_context = ""
                    if agent_communications:
                        collaboration_context += "\n\nAGENT COMMUNICATIONS:\n"
                        for comm in agent_communications[-2:]:
                            collaboration_context += f"- {comm['from']} ‚Üí {comm['to']}: {comm['message']}\n"
                    
                    if agent_responses:
                        collaboration_context += "\n\nPREVIOUS INSIGHTS:\n"
                        for prev_agent, response in list(agent_responses.items())[-1:]:
                            prev_role = available_agents[prev_agent].get('role', prev_agent)
                            
                            # CRITICAL FIX: PRESERVE thinking tags in collaboration context 
                            # This allows subsequent agents to see and build upon previous thinking
                            # Only truncate for length, don't strip thinking content
                            
                            # Smart truncation that preserves thinking tags
                            if len(response) > 500:  # Increased limit to preserve more context
                                # Find last complete sentence within 500 characters
                                truncated = response[:500]
                                last_period = truncated.rfind('.')
                                last_exclamation = truncated.rfind('!')
                                last_question = truncated.rfind('?')
                                
                                last_sentence_end = max(last_period, last_exclamation, last_question)
                                if last_sentence_end > 200:  # Ensure meaningful content
                                    summary = response[:last_sentence_end + 1]
                                else:
                                    # Fallback to word boundary
                                    last_space = truncated.rfind(' ')
                                    summary = response[:last_space] + "..." if last_space > 200 else response[:500] + "..."
                            else:
                                summary = response  # Keep full response if under limit
                                
                            collaboration_context += f"- {prev_role}: {summary}\n"
                    
                    # Check for relevant tools
                    relevant_tools = [tool for tool in agent_tools if tool in available_tools]
                    tool_context = ""
                    
                    if relevant_tools:
                        yield json.dumps({
                            "type": "agent_tool_start",
                            "agent": agent_name,
                            "tools": relevant_tools,
                            "message": f"üîß {agent_name} checking tools: {', '.join(relevant_tools)}"
                        }) + "\n"
                        
                        # REAL TOOL EXECUTION - Replace simulation with actual MCP tool calls
                        for tool in relevant_tools[:2]:
                            # Create tool execution span for tracing
                            tool_span = None
                            if agent_span and tracer.is_enabled():
                                try:
                                    tool_span = tracer.create_tool_span(
                                        agent_span,
                                        tool,
                                        {"query": question, "agent": agent_name},
                                        parent_span=agent_span
                                    )
                                    logger.info(f"Created tool span for {tool} used by {agent_name}")
                                except Exception as e:
                                    logger.warning(f"Failed to create tool span for {tool}: {e}")
                            
                            # Execute real MCP tool instead of simulation
                            tool_success = False
                            tool_result = ""
                            
                            try:
                                logger.info(f"üîß EXECUTING REAL TOOL: {tool} for {agent_name}")
                                
                                # Use threading to handle sync/async compatibility (pattern from dynamic_agent_system.py)
                                def execute_tool_sync():
                                    # Get tool info from MCP cache for proper parameters (no hardcode)
                                    tool_info = available_tools.get(tool, {})
                                    tool_parameters = tool_info.get('parameters', {})
                                    
                                    # Build parameters based on tool's JSON-RPC 2.0 schema
                                    tool_params = {}
                                    if tool_parameters:
                                        # For search tools, typically need 'query' parameter
                                        if 'query' in tool_parameters.get('properties', {}):
                                            tool_params['query'] = question
                                        if 'num_results' in tool_parameters.get('properties', {}):
                                            tool_params['num_results'] = 10
                                        # Add other common parameters based on schema
                                        for param_name, param_info in tool_parameters.get('properties', {}).items():
                                            if param_name not in tool_params:
                                                # Set reasonable defaults based on parameter type
                                                param_type = param_info.get('type', 'string')
                                                if param_type == 'integer' and 'count' in param_name:
                                                    tool_params[param_name] = 10
                                                elif param_type == 'boolean':
                                                    tool_params[param_name] = True
                                    else:
                                        # Fallback for tools without schema
                                        tool_params = {'query': question}
                                    
                                    return call_mcp_tool(tool, tool_params, trace=tool_span)
                                
                                # Execute tool in thread to avoid async conflicts
                                loop = asyncio.get_event_loop()
                                with ThreadPoolExecutor() as executor:
                                    future = loop.run_in_executor(executor, execute_tool_sync)
                                    tool_response = await asyncio.wait_for(future, timeout=30.0)
                                
                                # Use same success detection logic as standard chat
                                tool_success = True
                                if isinstance(tool_response, dict):
                                    if "error" in tool_response:
                                        tool_success = False
                                    elif "message" in tool_response and "error" in str(tool_response.get("message", "")).lower():
                                        tool_success = False
                                
                                if tool_success and tool_response:
                                    # Use tool response directly (same as standard chat pattern)
                                    # Don't try to extract nested content - the response IS the content
                                    tool_result = tool_response
                                    
                                    # Only convert to string for logging and context, not for actual result
                                    result_preview = str(tool_result)[:100]
                                    logger.info(f"‚úÖ Tool {tool} succeeded: {result_preview}...")
                                else:
                                    tool_result = tool_response.get('error', f"Tool {tool} failed") if isinstance(tool_response, dict) else f"Tool {tool} returned no results"
                                    logger.warning(f"‚ö†Ô∏è Tool {tool} failed: {tool_result}")
                                    
                            except asyncio.TimeoutError:
                                tool_result = f"Tool {tool} timed out after 30 seconds"
                                logger.error(f"‚ùå Tool {tool} timeout")
                            except Exception as e:
                                tool_result = f"Tool {tool} failed: {str(e)}"
                                logger.error(f"‚ùå Tool {tool} error: {e}")
                            
                            # Add tool result to context for LLM (convert to string for context)
                            tool_result_str = str(tool_result) if tool_result else "No result"
                            tool_context += f"\nTool {tool}: {tool_result_str}"
                            
                            # End tool span with real result
                            if tool_span and tracer.is_enabled():
                                try:
                                    tracer.end_span_with_result(
                                        tool_span,
                                        {
                                            "tool_name": tool,
                                            "result": tool_result,
                                            "agent": agent_name,
                                            "success": tool_success
                                        },
                                        success=tool_success
                                    )
                                except Exception as e:
                                    logger.warning(f"Failed to end tool span for {tool}: {e}")
                            
                            # Stream real tool result to UI
                            tool_result_preview = str(tool_result)[:200] + "..." if len(str(tool_result)) > 200 else str(tool_result)
                            yield json.dumps({
                                "type": "agent_tool_complete",
                                "agent": agent_name,
                                "tool": tool,
                                "result": tool_result_preview,
                                "success": tool_success
                            }) + "\n"
                            await asyncio.sleep(0.3)
                    
                    # Create comprehensive prompt
                    agent_prompt = f"""{system_prompt}

User Question: {question}{collaboration_context}{tool_context}

Your response should include detailed analysis with specific examples and evidence.

Available Tools: {', '.join(relevant_tools) if relevant_tools else 'None'}

Instructions:
1. Provide comprehensive analysis using your expertise (minimum 3-4 detailed paragraphs)
2. {"Reference and build upon previous agent insights" if collaboration_context else "Provide foundational analysis"}  
3. Include specific examples, case studies, and evidence
4. End with actionable recommendations and next steps
6. Ensure your response is thorough and adds substantial value to the analysis

IMPORTANT: This is a professional analysis that requires depth and detail. Provide at least 300-500 words of substantive content.

"""
                    
                    # Determine model and parameters based on configuration flags
                    if config.get('use_main_llm'):
                        from app.core.llm_settings_cache import get_main_llm_full_config
                        main_llm_config = get_main_llm_full_config()
                        agent_model = main_llm_config.get('model')
                        # Override with main_llm defaults if not specified in agent config
                        if 'max_tokens' not in config:
                            config['max_tokens'] = main_llm_config.get('max_tokens', 2000)
                        if 'temperature' not in config:
                            config['temperature'] = main_llm_config.get('temperature', 0.7)
                        logger.info(f"üîç {agent_name} using main_llm: {agent_model}")
                    elif config.get('use_second_llm') or not config.get('model'):
                        # Use second_llm explicitly or as default
                        second_llm_config = get_second_llm_full_config()
                        agent_model = second_llm_config.get('model')
                        # Override with second_llm defaults if not specified
                        if 'max_tokens' not in config:
                            config['max_tokens'] = second_llm_config.get('max_tokens', 2000)
                        if 'temperature' not in config:
                            config['temperature'] = second_llm_config.get('temperature', 0.7)
                        logger.info(f"üîç {agent_name} using second_llm: {agent_model}")
                    else:
                        # Use specific model
                        agent_model = config.get('model')
                        logger.info(f"üîç {agent_name} using specific model: {agent_model}")
                    
                    # Real LLM streaming call with agent-specific model
                    llm_config = LLMConfig(
                        model_name=agent_model,
                        temperature=config.get('temperature', 0.7),
                        top_p=config.get('top_p', 0.9),
                        max_tokens=config.get('max_tokens', 2000)
                    )
                    
                    # Create LLM generation span for tracing
                    if agent_span and tracer.is_enabled():
                        try:
                            agent_generation = tracer.create_llm_generation_span(
                                agent_span,
                                model=llm_config.model_name,
                                prompt=agent_prompt,
                                operation=f"agent_generation_{agent_name}"
                            )
                            logger.info(f"Created LLM generation span for {agent_name}")
                        except Exception as e:
                            logger.warning(f"Failed to create LLM generation span for {agent_name}: {e}")
                    
                    llm = OllamaLLM(llm_config, base_url=system.ollama_base_url)
                    
                    # Stream with thinking detection
                    full_response = ""
                    thinking_content = ""
                    in_thinking = False
                    thinking_detected = False
                    token_count = 0
                    
                    # Add debug logging
                    logger.info(f"Starting streaming for {agent_name}...")
                    
                    async for response_chunk in llm.generate_stream(agent_prompt):
                        chunk_text = response_chunk.text
                        full_response += chunk_text
                        token_count += 1
                        
                        # Log first few tokens for debugging
                        if token_count <= 5:
                            logger.info(f"[{agent_name}] Token {token_count}: '{chunk_text[:50]}...'")
                        
                        # Detect thinking start
                        if "<think>" in chunk_text.lower():
                            in_thinking = True
                            thinking_detected = True
                            
                            yield json.dumps({
                                "type": "agent_thinking_start",
                                "agent": agent_name,
                                "message": f"üí≠ {agent_name} reasoning..."
                            }) + "\n"
                            
                            # Extract content after <think>
                            start_idx = chunk_text.lower().find("<think>") + 7
                            thinking_content += chunk_text[start_idx:]
                            
                        # Detect thinking end
                        elif "</think>" in chunk_text.lower():
                            in_thinking = False
                            
                            # Extract content before </think>
                            end_idx = chunk_text.lower().find("</think>")
                            thinking_content += chunk_text[:end_idx]
                            
                            # Send complete thinking
                            yield json.dumps({
                                "type": "agent_thinking_complete",
                                "agent": agent_name,
                                "thinking": thinking_content.strip()
                            }) + "\n"
                            
                            # Reset thinking state
                            thinking_content = ""
                            
                            # CRITICAL FIX: Send the </think> tag as output to preserve complete response
                            yield json.dumps({
                                "type": "agent_token",
                                "agent": agent_name,
                                "token": "</think>"
                            }) + "\n"
                            
                            # Continue with remaining content after </think>
                            remaining = chunk_text[end_idx + 8:]
                            if remaining.strip():
                                yield json.dumps({
                                    "type": "agent_token",
                                    "agent": agent_name,
                                    "token": remaining
                                }) + "\n"
                        
                        # Accumulate thinking content
                        elif in_thinking:
                            thinking_content += chunk_text
                        
                        # Stream regular content (this should be the main path)
                        else:
                            if chunk_text.strip():
                                yield json.dumps({
                                    "type": "agent_token",
                                    "agent": agent_name,
                                    "token": chunk_text
                                }) + "\n"
                        
                        await asyncio.sleep(0.02)
                    
                    logger.info(f"Streaming completed for {agent_name}. Total tokens: {token_count}, thinking_detected: {thinking_detected}")
                    
                    # Fallback thinking if none detected
                    logger.info(f"[DEBUG] {agent_name}: thinking_detected = {thinking_detected}")
                    if not thinking_detected:
                        fallback_thinking = f"Analyzing '{question}' from {role} perspective. I need to consider my domain expertise, available insights from other agents, and provide actionable recommendations based on my specialized knowledge."
                        
                        logger.info(f"[DEBUG] {agent_name}: Sending fallback thinking")
                        yield json.dumps({
                            "type": "agent_thinking_start",
                            "agent": agent_name,
                            "message": f"üí≠ {agent_name} reasoning..."
                        }) + "\n"
                        
                        yield json.dumps({
                            "type": "agent_thinking_complete",
                            "agent": agent_name,
                            "thinking": fallback_thinking
                        }) + "\n"
                    else:
                        logger.info(f"[DEBUG] {agent_name}: Thinking was detected, not sending fallback")
                    
                    # CRITICAL FIX: If no tokens were streamed but we have a response, send it now
                    if token_count == 0 and full_response.strip():
                        logger.warning(f"[CRITICAL] {agent_name}: No tokens streamed but response exists! Sending full response now.")
                        yield json.dumps({
                            "type": "agent_token",
                            "agent": agent_name,
                            "token": full_response
                        }) + "\n"
                    elif token_count > 0:
                        logger.info(f"[SUCCESS] {agent_name}: Streamed {token_count} tokens successfully")
                    else:
                        logger.error(f"[ERROR] {agent_name}: No response generated at all!")
                    
                    # Store response for collaboration
                    agent_response = system._clean_llm_response(full_response)
                    agent_responses[agent_name] = agent_response
                    
                    # Record performance metrics
                    try:
                        agent_execution_time = time.time() - agent_start_time
                        performance_tracker.record_agent_execution(
                            agent_name=agent_name,
                            success=True,
                            response_length=len(agent_response),
                            execution_time=agent_execution_time,
                            question_complexity=question_analysis.complexity,
                            question_domain=question_analysis.domain,
                            collaboration_mode=question_analysis.collaboration_type
                        )
                    except Exception as perf_error:
                        logger.warning(f"Failed to record performance metrics for {agent_name}: {perf_error}")
                    
                    # End LLM generation span with results - use full_response to preserve thinking tags
                    if agent_generation and tracer.is_enabled():
                        try:
                            usage = tracer.estimate_token_usage(agent_prompt, full_response)
                            agent_generation.end(
                                output=full_response,  # Use full_response to show complete thinking in Langfuse
                                usage=usage,
                                metadata={
                                    "agent_name": agent_name,
                                    "thinking_detected": thinking_detected,
                                    "tools_used": relevant_tools,
                                    "collaboration_context_length": len(collaboration_context),
                                    "response_length": len(full_response),
                                    "operation": f"agent_generation_{agent_name}"
                                }
                            )
                            logger.info(f"Ended LLM generation span for {agent_name}")
                        except Exception as e:
                            logger.warning(f"Failed to end LLM generation span for {agent_name}: {e}")
                    
                    # Send communication to next agent
                    if i < len(agents_used) - 1:
                        next_agent = agents_used[i + 1]
                        next_agent_data = available_agents[next_agent]
                        next_role = next_agent_data.get('role', next_agent)
                        
                        # IMPROVED: Clean thinking tags for communication messages (but preserve structure)
                        # Communication messages should be concise summaries, not full thinking content
                        clean_response = re.sub(r'<think>.*?</think>', '', agent_response, flags=re.DOTALL | re.IGNORECASE).strip()
                        clean_response = re.sub(r'</?think>', '', clean_response, flags=re.IGNORECASE).strip()
                        
                        # Additional cleanup for better communication messages
                        clean_response = re.sub(r'\n\s*\n+', ' ', clean_response)  # Collapse multiple newlines
                        
                        # Smart truncation that preserves sentence integrity
                        if len(clean_response) > 400:
                            # Find last complete sentence within 400 characters
                            truncated = clean_response[:400]
                            last_period = truncated.rfind('.')
                            last_exclamation = truncated.rfind('!')
                            last_question = truncated.rfind('?')
                            
                            # Use the last complete sentence boundary
                            last_sentence_end = max(last_period, last_exclamation, last_question)
                            if last_sentence_end > 200:  # Ensure we have meaningful content
                                insights_summary = clean_response[:last_sentence_end + 1]
                            else:
                                # Fallback to word boundary
                                truncated = clean_response[:400]
                                last_space = truncated.rfind(' ')
                                insights_summary = clean_response[:last_space] + "..." if last_space > 200 else clean_response[:400] + "..."
                        else:
                            insights_summary = clean_response
                        
                        comm_message = f"Key insights from {role}: {insights_summary} Please build upon this with your expertise."
                        
                        agent_communications.append({
                            "from": agent_name,
                            "to": next_agent,
                            "message": comm_message,
                            "summary": insights_summary,
                            "handoff_reason": f"Transitioning from {role} analysis to {next_role} perspective"
                        })
                        
                        yield json.dumps({
                            "type": "agent_communication",
                            "from": agent_name,
                            "from_role": role,
                            "to": next_agent,
                            "to_role": next_role,
                            "communication": {
                                "message": comm_message,
                                "summary": insights_summary,
                                "handoff_reason": f"Transitioning from {role} analysis to {next_role} perspective",
                                "context_shared": True
                            },
                            "progress": {
                                "handoff_step": i + 1,
                                "total_handoffs": len(agents_used) - 1
                            },
                            "timestamp": "now",
                            "message": f"üîÑ Handing off from {role} to {next_role}..."
                        }) + "\n"
                    
                    # End agent execution span with results
                    if agent_span and tracer.is_enabled():
                        try:
                            tracer.end_span_with_result(
                                agent_span,
                                {
                                    "agent_name": agent_name,
                                    "response_length": len(agent_response),
                                    "tools_used": relevant_tools,
                                    "thinking_detected": thinking_detected,
                                    "collaboration_used": bool(collaboration_context),
                                    "success": True
                                },
                                success=True
                            )
                            logger.info(f"Ended agent execution span for {agent_name}")
                        except Exception as e:
                            logger.warning(f"Failed to end agent execution span for {agent_name}: {e}")
                    
                    # Send agent completion signal with full response including thinking tags
                    logger.info(f"Sending agent_complete for {agent_name} with response length: {len(agent_response)}")
                    logger.info(f"Agent response contains thinking tags: {'<think>' in agent_response and '</think>' in agent_response}")
                    
                    yield json.dumps({
                        "type": "agent_complete",
                        "agent": agent_name,
                        "content": agent_response,  # This includes thinking tags preserved by _clean_llm_response
                        "agent_info": {
                            "role": role,
                            "position": i + 1,
                            "total_agents": len(agents_used),
                            "tools_used": relevant_tools,
                            "thinking_detected": thinking_detected,
                            "response_length": len(agent_response)
                        },
                        "progress": {
                            "current_step": i + 1,
                            "total_steps": len(agents_used) + 1,
                            "percentage": int(((i + 1) / (len(agents_used) + 1)) * 100),
                            "phase_name": f"Agent {i + 1} Complete",
                            "remaining_agents": len(agents_used) - (i + 1)
                        },
                        "message": f"‚úÖ {agent_name} analysis complete ({i + 1}/{len(agents_used)})"
                    }) + "\n"
                    
                except Exception as e:
                    logger.error(f"Agent {agent_name} failed: {e}")
                    fallback_response = f"Analysis from {agent_name}: {question} requires careful consideration from {role} perspective."
                    agent_responses[agent_name] = fallback_response
                    
                    # Record failed performance metrics
                    try:
                        agent_execution_time = time.time() - agent_start_time
                        performance_tracker.record_agent_execution(
                            agent_name=agent_name,
                            success=False,
                            response_length=len(fallback_response),
                            execution_time=agent_execution_time,
                            question_complexity=question_analysis.complexity,
                            question_domain=question_analysis.domain,
                            collaboration_mode=question_analysis.collaboration_type
                        )
                    except Exception as perf_error:
                        logger.warning(f"Failed to record failure metrics for {agent_name}: {perf_error}")
                    
                    # End spans with error for failed agents
                    if agent_generation and tracer.is_enabled():
                        try:
                            usage = tracer.estimate_token_usage(agent_prompt if 'agent_prompt' in locals() else question, fallback_response)
                            agent_generation.end(
                                output=f"Error: {str(e)}",
                                usage=usage,
                                metadata={
                                    "agent_name": agent_name,
                                    "error": str(e),
                                    "fallback_used": True,
                                    "operation": f"agent_generation_{agent_name}"
                                }
                            )
                        except Exception as trace_error:
                            logger.warning(f"Failed to end LLM generation span with error for {agent_name}: {trace_error}")
                    
                    if agent_span and tracer.is_enabled():
                        try:
                            tracer.end_span_with_result(
                                agent_span,
                                {
                                    "agent_name": agent_name,
                                    "error": str(e),
                                    "fallback_response": fallback_response,
                                    "success": False
                                },
                                success=False,
                                error=str(e)
                            )
                        except Exception as trace_error:
                            logger.warning(f"Failed to end agent execution span with error for {agent_name}: {trace_error}")
                    
                    yield json.dumps({
                        "type": "agent_complete",
                        "agent": agent_name,
                        "content": fallback_response,
                        "agent_info": {
                            "role": role,
                            "position": i + 1,
                            "total_agents": len(agents_used),
                            "tools_used": relevant_tools,
                            "thinking_detected": False,
                            "response_length": len(fallback_response),
                            "status": "fallback"
                        },
                        "progress": {
                            "current_step": i + 1,
                            "total_steps": len(agents_used) + 1,
                            "percentage": int(((i + 1) / (len(agents_used) + 1)) * 100),
                            "phase_name": f"Agent {i + 1} Complete",
                            "remaining_agents": len(agents_used) - (i + 1)
                        },
                        "message": f"‚úÖ {agent_name} analysis complete (fallback) ({i + 1}/{len(agents_used)})"
                    }) + "\n"
                
                await asyncio.sleep(0.2)
            
            # Synthesis phase with detailed progress
            yield json.dumps({
                "type": "synthesis_start",
                "phase": "synthesis",
                "synthesis_info": {
                    "total_agents_processed": len(agents_used),
                    "communications_count": len(agent_communications),
                    "synthesis_type": "collaborative_integration",
                    "estimated_time": "2-3 min"
                },
                "progress": {
                    "current_step": len(agents_used) + 1,
                    "total_steps": len(agents_used) + 1,
                    "percentage": 80,  # Start synthesis at 80%
                    "phase_name": "Synthesis Phase - Gathering Insights",
                    "status": "gathering_insights",
                    "activity": "Collecting and organizing agent responses"
                },
                "message": "üîÆ Synthesizing collaborative insights from all agents..."
            }) + "\n"
            await asyncio.sleep(0.5)
            
            # Synthesis sub-phase 1: Gathering insights (85%)
            yield json.dumps({
                "type": "synthesis_progress",
                "progress": {
                    "current_step": len(agents_used) + 1,
                    "total_steps": len(agents_used) + 1,
                    "percentage": 85,
                    "phase_name": "Synthesis Phase - Analyzing Insights",
                    "status": "analyzing_insights",
                    "activity": "Analyzing agent communications and building integration framework"
                },
                "synthesis_details": {
                    "agents_being_integrated": agents_used,
                    "integration_steps": [
                        f"‚úÖ Collected insights from {agent}" for agent in agents_used
                    ] + ["üîÑ Building integration framework"],
                    "current_focus": "Agent communication patterns"
                },
                "message": "üîç Analyzing agent insights and communication patterns..."
            }) + "\n"
            await asyncio.sleep(0.3)
            
            # Create synthesis prompt
            synthesis_prompt = f"""
I need to synthesize the collaborative analysis from {len(agents_used)} agents who worked together on: {question}

Let me review their communications and create an integrated response.


COLLABORATION SUMMARY:
"""
            
            for comm in agent_communications:
                synthesis_prompt += f"Communication: {comm['from']} ‚Üí {comm['to']}: {comm['message']}\n"
            
            synthesis_prompt += f"""

AGENT ANALYSES:
"""
            
            for agent_name, response in agent_responses.items():
                role = available_agents[agent_name].get('role', agent_name)
                synthesis_prompt += f"\n{role}: {response}\n"
            
            synthesis_prompt += """

SYNTHESIS TASK:
Create a comprehensive, detailed synthesis response that:
1. Integrates key insights from all agents
2. Shows how agents collaborated and built upon each other's work
3. Provides actionable recommendations
4. Is at least 3-4 paragraphs long with specific examples

Begin your synthesis now:"""
            
            # Debug: Log synthesis prompt length
            prompt_length = len(synthesis_prompt)
            print(f"[DEBUG] Synthesis prompt length: {prompt_length} characters")
            if prompt_length > 20000:
                print(f"[WARNING] Very long synthesis prompt ({prompt_length} chars) - may cause truncation")
                # Truncate agent responses if needed to prevent context overflow
                max_response_length = 1000
                truncated_prompt = f"""I need to synthesize the collaborative analysis from {len(agents_used)} agents who worked together on: {question}

Let me review their communications and create an integrated response.

COLLABORATION SUMMARY:
"""
                for comm in agent_communications:
                    truncated_prompt += f"Communication: {comm['from']} ‚Üí {comm['to']}: {comm['message']}\n"
                
                truncated_prompt += "\n\nAGENT ANALYSES:\n"
                
                for agent_name, response in agent_responses.items():
                    role = available_agents[agent_name].get('role', agent_name)
                    truncated_response = response[:max_response_length] + "..." if len(response) > max_response_length else response
                    truncated_prompt += f"\n{role}: {truncated_response}\n"
                
                truncated_prompt += """

SYNTHESIS TASK:
Create a comprehensive, detailed synthesis response that:
1. Integrates key insights from all agents
2. Shows how agents collaborated and built upon each other's work
3. Provides actionable recommendations
4. Is at least 3-4 paragraphs long with specific examples

Begin your synthesis now:"""
                
                synthesis_prompt = truncated_prompt
                print(f"[DEBUG] Truncated synthesis prompt to {len(synthesis_prompt)} characters")
            
            # Synthesis sub-phase 2: Building integration (90%)
            yield json.dumps({
                "type": "synthesis_progress",
                "progress": {
                    "current_step": len(agents_used) + 1,
                    "total_steps": len(agents_used) + 1,
                    "percentage": 90,
                    "phase_name": "Synthesis Phase - Building Integration",
                    "status": "building_integration",
                    "activity": "Creating unified response from collaborative insights"
                },
                "synthesis_details": {
                    "agents_being_integrated": agents_used,
                    "integration_steps": [
                        f"‚úÖ Integrated insights from {agent}" for agent in agents_used
                    ] + ["üîÑ Creating unified narrative"],
                    "current_focus": "Cross-agent insight synthesis"
                },
                "message": "üî® Building integrated response from agent collaborations..."
            }) + "\n"
            await asyncio.sleep(0.3)
            
            # Create synthesis span for tracing
            synthesis_span = None
            synthesis_generation = None
            if workflow_span and tracer.is_enabled():
                try:
                    synthesis_span = tracer.create_span(
                        workflow_span,
                        name="synthesis",
                        metadata={
                            "operation": "multi_agent_synthesis",
                            "agent_count": len(agents_used),
                            "communication_count": len(agent_communications),
                            "synthesis_type": "collaborative_integration"
                        }
                    )
                    logger.info("Created synthesis span for multi-agent workflow")
                except Exception as e:
                    logger.warning(f"Failed to create synthesis span: {e}")
            
            # Synthesis streaming with thinking model preference
            try:
                # Get synthesizer configuration first
                synthesizer_agent = get_agent_by_name("synthesizer")
                synthesis_model = None
                temperature = 0.6
                max_tokens = 2500
                
                if synthesizer_agent:
                    synthesizer_config_data = synthesizer_agent.get('config', {})
                    
                    # Check synthesizer's LLM configuration
                    if synthesizer_config_data.get('use_main_llm'):
                        from app.core.llm_settings_cache import get_main_llm_full_config
                        main_llm_config = get_main_llm_full_config()
                        synthesis_model = main_llm_config.get('model')
                        temperature = synthesizer_config_data.get('temperature', main_llm_config.get('temperature', 0.6))
                        max_tokens = synthesizer_config_data.get('max_tokens', main_llm_config.get('max_tokens', 2500))
                    elif synthesizer_config_data.get('use_second_llm'):
                        second_llm_config = get_second_llm_full_config()
                        synthesis_model = second_llm_config.get('model')
                        temperature = synthesizer_config_data.get('temperature', second_llm_config.get('temperature', 0.6))
                        max_tokens = synthesizer_config_data.get('max_tokens', second_llm_config.get('max_tokens', 2500))
                    elif synthesizer_config_data.get('model'):
                        synthesis_model = synthesizer_config_data.get('model')
                        temperature = synthesizer_config_data.get('temperature', 0.6)
                        max_tokens = synthesizer_config_data.get('max_tokens', 2500)
                
                # If no synthesizer config found, fall back to second_llm
                if not synthesis_model:
                    second_llm_config = get_second_llm_full_config()
                    synthesis_model = second_llm_config.get('model', 'qwen3:30b-a3b')
                
                logger.info(f"Synthesizer using model: {synthesis_model}")
                
                synthesizer_config = LLMConfig(
                    model_name=synthesis_model,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                # Create synthesis generation span
                if synthesis_span and tracer.is_enabled():
                    try:
                        synthesis_generation = tracer.create_llm_generation_span(
                            synthesis_span,
                            model=synthesizer_config.model_name,
                            prompt=synthesis_prompt,
                            operation="synthesis_generation"
                        )
                        logger.info("Created synthesis generation span")
                    except Exception as e:
                        logger.warning(f"Failed to create synthesis generation span: {e}")
                
                synthesizer_llm = OllamaLLM(synthesizer_config, base_url=system.ollama_base_url)
                
                final_response = ""
                thinking_content = ""
                in_thinking = False
                synthesis_thinking_detected = False
                
                async for response_chunk in synthesizer_llm.generate_stream(synthesis_prompt):
                    chunk_text = response_chunk.text
                    final_response += chunk_text
                    
                    if "<think>" in chunk_text.lower():
                        in_thinking = True
                        synthesis_thinking_detected = True
                        
                        yield json.dumps({
                            "type": "synthesis_thinking_start",
                            "agent": "synthesizer",
                            "message": "üí≠ Synthesizing insights..."
                        }) + "\n"
                        
                        start_idx = chunk_text.lower().find("<think>") + 7
                        thinking_content += chunk_text[start_idx:]
                        
                    elif "</think>" in chunk_text.lower():
                        in_thinking = False
                        
                        end_idx = chunk_text.lower().find("</think>")
                        thinking_content += chunk_text[:end_idx]
                        
                        yield json.dumps({
                            "type": "synthesis_thinking_complete",
                            "agent": "synthesizer",
                            "thinking": thinking_content.strip()
                        }) + "\n"
                        
                        # CRITICAL FIX: Send the </think> tag as output to preserve complete response
                        yield json.dumps({
                            "type": "agent_token",
                            "agent": "synthesizer",
                            "token": "</think>"
                        }) + "\n"
                        
                        remaining = chunk_text[end_idx + 8:]
                        if remaining.strip():
                            yield json.dumps({
                                "type": "agent_token",
                                "agent": "synthesizer",
                                "token": remaining
                            }) + "\n"
                            
                    elif in_thinking:
                        thinking_content += chunk_text
                        
                    else:
                        if chunk_text.strip():
                            yield json.dumps({
                                "type": "agent_token",
                                "agent": "synthesizer",
                                "token": chunk_text
                            }) + "\n"
                    
                    await asyncio.sleep(0.02)
                
                final_answer = system._clean_llm_response(final_response)
                print(f"[DEBUG] Synthesis complete - final_response length: {len(final_response)}, final_answer length: {len(final_answer)}")
                
                # Synthesis completion: 100% with all steps completed
                yield json.dumps({
                    "type": "synthesis_progress",
                    "progress": {
                        "current_step": len(agents_used) + 1,
                        "total_steps": len(agents_used) + 1,
                        "percentage": 100,
                        "phase_name": "Synthesis Phase - Complete",
                        "status": "synthesis_complete",
                        "activity": "Synthesis successfully completed"
                    },
                    "synthesis_details": {
                        "agents_being_integrated": agents_used,
                        "integration_steps": [
                            f"‚úÖ Completed insights from {agent}" for agent in agents_used
                        ] + ["‚úÖ Unified narrative complete", "‚úÖ Final polish and formatting complete"],
                        "current_focus": "Synthesis complete"
                    },
                    "message": "‚úÖ Synthesis phase completed successfully!"
                }) + "\n"
                await asyncio.sleep(0.2)
                
                
                # End synthesis generation span with results
                if synthesis_generation and tracer.is_enabled():
                    try:
                        usage = tracer.estimate_token_usage(synthesis_prompt, final_response)
                        synthesis_generation.end(
                            output=final_response,  # Use final_response to show complete thinking in Langfuse
                            usage=usage,
                            metadata={
                                "synthesis_thinking_detected": synthesis_thinking_detected,
                                "response_length": len(final_response),
                                "agent_count": len(agents_used),
                                "operation": "synthesis_generation"
                            }
                        )
                        logger.info("Ended synthesis generation span")
                    except Exception as e:
                        logger.warning(f"Failed to end synthesis generation span: {e}")
                
            except Exception as e:
                logger.error(f"Synthesis failed: {e}")
                final_answer = f"Collaborative analysis from {', '.join(agents_used)} provides comprehensive insights."
                
                # End synthesis spans with error
                if synthesis_generation and tracer.is_enabled():
                    try:
                        usage = tracer.estimate_token_usage(synthesis_prompt, final_answer)
                        synthesis_generation.end(
                            output=f"Error: {str(e)}",
                            usage=usage,
                            metadata={
                                "error": str(e),
                                "fallback_used": True,
                                "operation": "synthesis_generation"
                            }
                        )
                    except Exception as trace_error:
                        logger.warning(f"Failed to end synthesis generation span with error: {trace_error}")
            
            # End synthesis span
            if synthesis_span and tracer.is_enabled():
                try:
                    tracer.end_span_with_result(
                        synthesis_span,
                        {
                            "final_response_length": len(final_answer),
                            "agent_count": len(agents_used),
                            "synthesis_success": 'final_answer' in locals() and len(final_answer) > 50,
                            "collaboration_summary": {
                                "agent_communications": len(agent_communications),
                                "thinking_sessions": len(agents_used) + 1
                            }
                        },
                        success=True
                    )
                    logger.info("Ended synthesis span")
                except Exception as e:
                    logger.warning(f"Failed to end synthesis span: {e}")
            
            # Final response with enhanced collaboration metrics
            yield json.dumps({
                "type": "final_response",
                "response": final_answer,
                "execution_summary": {
                    "agents_used": agents_used,
                    "total_agents": len(agents_used),
                    "execution_pattern": "collaborative_with_thinking_and_tools",
                    "completion_status": "success"
                },
                "collaboration_summary": {
                    "agent_communications": len(agent_communications),
                    "successful_handoffs": len(agent_communications),
                    "tools_used": sum(len(available_agents[agent].get('tools', [])) for agent in agents_used),
                    "thinking_sessions": len(agents_used) + 1,  # +1 for synthesizer
                    "total_processing_phases": len(agents_used) + 1
                },
                "progress": {
                    "current_step": len(agents_used) + 1,
                    "total_steps": len(agents_used) + 1,
                    "percentage": 100,
                    "phase_name": "Complete",
                    "status": "success"
                },
                "performance_metrics": {
                    "confidence_score": 0.95,
                    "response_length": len(final_answer),
                    "synthesis_success": True
                },
                "conversation_id": conversation_id,
                "message": f"üéâ Collaborative analysis complete! {len(agents_used)} agents collaborated successfully."
            }) + "\n"
            
            # End workflow span with success
            if workflow_span and tracer.is_enabled():
                try:
                    tracer.end_span_with_result(
                        workflow_span,
                        {
                            "agents_used": agents_used,
                            "final_response_length": len(final_answer),
                            "collaboration_metrics": {
                                "agent_count": len(agents_used),
                                "communications": len(agent_communications),
                                "successful_synthesis": True
                            },
                            "execution_pattern": "collaborative_with_thinking_and_tools",
                            "confidence_score": 0.95
                        },
                        success=True
                    )
                    logger.info("Ended multi-agent workflow span successfully")
                except Exception as e:
                    logger.warning(f"Failed to end workflow span: {e}")
            
        except Exception as e:
            logger.error(f"Fixed multi-agent streaming failed: {e}")
            
            # End workflow span with error
            if workflow_span and tracer.is_enabled():
                try:
                    tracer.end_span_with_result(
                        workflow_span,
                        {
                            "error": str(e),
                            "agents_attempted": agents_used if 'agents_used' in locals() else [],
                            "failure_point": "workflow_execution"
                        },
                        success=False,
                        error=str(e)
                    )
                    logger.info("Ended multi-agent workflow span with error")
                except Exception as trace_error:
                    logger.warning(f"Failed to end workflow span with error: {trace_error}")
            
            yield json.dumps({
                "type": "error",
                "error": str(e),
                "message": "‚ùå Multi-agent execution failed"
            }) + "\n"
    
    return stream_fixed_multi_agent()