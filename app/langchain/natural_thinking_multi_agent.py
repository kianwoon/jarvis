"""
Natural thinking multi-agent streaming that works with qwen3:30b-a3b thinking model
Lets the model naturally produce <think> content without forcing it
"""
import json
import asyncio
from typing import Optional
from app.langchain.langgraph_multi_agent_system import LangGraphMultiAgentSystem
from app.llm.ollama import OllamaLLM
from app.llm.base import LLMConfig
from app.core.mcp_tools_cache import get_enabled_mcp_tools
import logging

logger = logging.getLogger(__name__)

async def natural_thinking_multi_agent_streaming(
    question: str,
    conversation_id: Optional[str] = None
):
    """
    Natural thinking multi-agent streaming that works with qwen3:30b-a3b
    Lets the model naturally produce thinking content without forcing it
    """
    
    async def stream_natural_thinking():
        try:
            yield json.dumps({"type": "status", "message": "üéØ Starting natural thinking multi-agent analysis..."}) + "\n"
            await asyncio.sleep(0.3)
            
            # Initialize system
            system = LangGraphMultiAgentSystem(conversation_id)
            
            # Get agents from database and tools
            available_agents = system.agents
            available_tools = get_enabled_mcp_tools()
            
            if not available_agents:
                yield json.dumps({"type": "error", "error": "No agents found in database"}) + "\n"
                return
            
            # Select agents based on their tools and relevance
            question_lower = question.lower()
            selected_agents = []
            
            # Prioritize agents with tools
            agents_with_tools = []
            agents_without_tools = []
            
            for agent_name, agent_data in available_agents.items():
                tools = agent_data.get('tools', [])
                role = agent_data.get('role', '').lower()
                
                if tools and any(tool in available_tools for tool in tools):
                    agents_with_tools.append((agent_name, agent_data))
                else:
                    agents_without_tools.append((agent_name, agent_data))
            
            # Select mix of agents with and without tools
            selected_agents = []
            if agents_with_tools:
                selected_agents.extend([agent[0] for agent in agents_with_tools[:2]])
            if agents_without_tools:
                selected_agents.extend([agent[0] for agent in agents_without_tools[:2]])
            
            # Ensure we have at least 2 agents
            if len(selected_agents) < 2:
                selected_agents = list(available_agents.keys())[:3]
            
            agents_used = selected_agents[:3]  # Limit to 3 agents
            
            yield json.dumps({
                "type": "agents_selected", 
                "agents": agents_used,
                "message": f"üéØ Selected agents: {', '.join(agents_used)}"
            }) + "\n"
            await asyncio.sleep(0.2)
            
            # Track responses for collaboration
            agent_responses = {}
            agent_communications = []
            
            # Process each agent
            for i, agent_name in enumerate(agents_used):
                yield json.dumps({
                    "type": "agent_start",
                    "agent": agent_name,
                    "message": f"üéØ {agent_name} beginning analysis..."
                }) + "\n"
                await asyncio.sleep(0.3)
                
                try:
                    # Get agent config from database
                    agent_data = available_agents[agent_name]
                    system_prompt = agent_data.get('system_prompt', '')
                    role = agent_data.get('role', agent_name)
                    config = agent_data.get('config', {})
                    agent_tools = agent_data.get('tools', [])
                    
                    # Build collaborative context
                    collaboration_context = ""
                    if agent_responses:
                        collaboration_context += "\n\nPrevious agent insights:\n"
                        for prev_agent, prev_response in list(agent_responses.items())[-1:]:
                            prev_role = available_agents[prev_agent].get('role', prev_agent)
                            summary = prev_response[:250] + "..." if len(prev_response) > 250 else prev_response
                            collaboration_context += f"- {prev_role}: {summary}\n"
                        collaboration_context += "\nBuild upon these insights with your expertise.\n"
                    
                    # Check available tools
                    relevant_tools = [tool for tool in agent_tools if tool in available_tools]
                    tool_context = ""
                    tool_results = {}
                    
                    if relevant_tools:
                        yield json.dumps({
                            "type": "agent_tool_start",
                            "agent": agent_name,
                            "tools": relevant_tools,
                            "message": f"üîß {agent_name} has tools: {', '.join(relevant_tools)}"
                        }) + "\n"
                        
                        # Execute relevant tools
                        for tool in relevant_tools[:2]:  # Limit to 2 tools per agent
                            if ("search" in tool.lower() and 
                                any(keyword in question_lower for keyword in ["research", "find", "information", "data", "study"])):
                                
                                # Simulate search tool execution
                                search_result = f"Found relevant research data and industry reports about '{question}' from multiple authoritative sources."
                                tool_results[tool] = search_result
                                tool_context += f"\nTool {tool} found: {search_result}\n"
                                
                                yield json.dumps({
                                    "type": "agent_tool_complete",
                                    "agent": agent_name,
                                    "tool": tool,
                                    "result": search_result
                                }) + "\n"
                                await asyncio.sleep(0.3)
                    
                    # Create natural prompt - let qwen3:30b-a3b think naturally
                    agent_prompt = f"""{system_prompt}

User Question: {question}{collaboration_context}{tool_context}

As a {role}, analyze this question and provide comprehensive insights. The qwen3 thinking model will naturally show reasoning process.

Available tools: {', '.join(relevant_tools) if relevant_tools else 'None'}

Provide detailed analysis with specific examples and actionable recommendations."""
                    
                    # Real LLM streaming with natural thinking detection
                    llm_config = LLMConfig(
                        model_name=system.llm_settings["model"],
                        temperature=config.get('temperature', 0.7),
                        top_p=config.get('top_p', 0.9),
                        max_tokens=config.get('max_tokens', 2000)
                    )
                    
                    llm = OllamaLLM(llm_config, base_url=system.ollama_base_url)
                    
                    # Stream with natural thinking detection
                    full_response = ""
                    current_thinking = ""
                    in_thinking = False
                    thinking_detected = False
                    
                    async for response_chunk in llm.generate_stream(agent_prompt):
                        chunk_text = response_chunk.text
                        full_response += chunk_text
                        
                        # Natural thinking detection
                        if "<think>" in chunk_text:
                            in_thinking = True
                            thinking_detected = True
                            
                            yield json.dumps({
                                "type": "agent_thinking_start",
                                "agent": agent_name,
                                "message": f"üí≠ {agent_name} thinking..."
                            }) + "\n"
                            
                            # Start collecting thinking content
                            think_start = chunk_text.find("<think>") + 7
                            current_thinking = chunk_text[think_start:]
                            
                        elif "</think>" in chunk_text:
                            in_thinking = False
                            
                            # Complete thinking collection
                            think_end = chunk_text.find("</think>")
                            current_thinking += chunk_text[:think_end]
                            
                            # Send complete thinking
                            yield json.dumps({
                                "type": "agent_thinking_complete",
                                "agent": agent_name,
                                "thinking": current_thinking.strip()
                            }) + "\n"
                            
                            # Continue with regular content
                            remaining_content = chunk_text[think_end + 8:]
                            if remaining_content.strip():
                                yield json.dumps({
                                    "type": "agent_token",
                                    "agent": agent_name,
                                    "token": remaining_content
                                }) + "\n"
                        
                        elif in_thinking:
                            # Accumulate thinking content
                            current_thinking += chunk_text
                        
                        else:
                            # Stream regular content tokens
                            if chunk_text.strip():
                                yield json.dumps({
                                    "type": "agent_token",
                                    "agent": agent_name,
                                    "token": chunk_text
                                }) + "\n"
                        
                        await asyncio.sleep(0.02)
                    
                    # Fallback thinking if none detected
                    if not thinking_detected:
                        fallback_thinking = f"Analyzing '{question}' from {role} perspective. I need to consider my domain expertise, available insights from other agents, and provide actionable recommendations based on my specialized knowledge."
                        
                        yield json.dumps({
                            "type": "agent_thinking_start",
                            "agent": agent_name,
                            "message": f"üí≠ {agent_name} reasoning..."
                        }) + "\n"
                        
                        yield json.dumps({
                            "type": "agent_thinking_complete",
                            "agent": agent_name,
                            "thinking": fallback_thinking
                        }) + "\n"
                    
                    # Store response for collaboration
                    clean_response = system._clean_llm_response(full_response)
                    agent_responses[agent_name] = clean_response
                    
                    # Create communication to next agent
                    if i < len(agents_used) - 1:
                        next_agent = agents_used[i + 1]
                        next_role = available_agents[next_agent].get('role', next_agent)
                        
                        comm_message = f"From {role}: {clean_response[:200]}... Please analyze from {next_role} perspective."
                        
                        agent_communications.append({
                            "from": agent_name,
                            "to": next_agent,
                            "message": comm_message
                        })
                        
                        yield json.dumps({
                            "type": "agent_communication",
                            "from": agent_name,
                            "to": next_agent,
                            "message": comm_message,
                            "timestamp": "now"
                        }) + "\n"
                    
                    yield json.dumps({
                        "type": "agent_complete",
                        "agent": agent_name,
                        "content": clean_response,
                        "tools_used": list(tool_results.keys()),
                        "thinking_detected": thinking_detected,
                        "message": f"‚úÖ {agent_name} completed analysis"
                    }) + "\n"
                    
                except Exception as e:
                    logger.error(f"Agent {agent_name} failed: {e}")
                    # Fallback response
                    fallback = f"Analysis from {role}: {question} requires comprehensive evaluation from multiple perspectives."
                    agent_responses[agent_name] = fallback
                    
                    yield json.dumps({
                        "type": "agent_complete",
                        "agent": agent_name,
                        "content": fallback,
                        "message": f"‚úÖ {agent_name} completed (fallback)"
                    }) + "\n"
                
                await asyncio.sleep(0.2)
            
            # Final synthesis
            yield json.dumps({
                "type": "status",
                "message": "üîÆ Synthesizing multi-agent insights..."
            }) + "\n"
            await asyncio.sleep(0.5)
            
            # Create synthesis prompt
            synthesis_prompt = f"""Question: {question}

Multi-agent collaboration results:
"""
            
            for agent_name, response in agent_responses.items():
                role = available_agents[agent_name].get('role', agent_name)
                synthesis_prompt += f"\n{role} Analysis:\n{response}\n"
            
            if agent_communications:
                synthesis_prompt += f"\nAgent Communications:\n"
                for comm in agent_communications:
                    synthesis_prompt += f"- {comm['from']} ‚Üí {comm['to']}: {comm['message']}\n"
            
            synthesis_prompt += f"\nSynthesize these collaborative insights into a comprehensive, integrated response that shows how the agents worked together."
            
            # Synthesis with natural thinking
            try:
                # Get synthesizer agent configuration
                from app.core.langgraph_agents_cache import get_agent_by_name
                synthesizer_agent = get_agent_by_name("synthesizer")
                
                # Determine model and parameters based on synthesizer configuration
                if synthesizer_agent:
                    agent_config = synthesizer_agent.get('config', {})
                    
                    if agent_config.get('use_main_llm'):
                        from app.core.llm_settings_cache import get_main_llm_full_config
                        main_llm_config = get_main_llm_full_config()
                        model_name = main_llm_config.get('model')
                        temperature = agent_config.get('temperature', main_llm_config.get('temperature', 0.6))
                        max_tokens = agent_config.get('max_tokens', main_llm_config.get('max_tokens', 2500))
                    elif agent_config.get('use_second_llm'):
                        from app.core.llm_settings_cache import get_second_llm_full_config
                        second_llm_config = get_second_llm_full_config()
                        model_name = second_llm_config.get('model')
                        temperature = agent_config.get('temperature', second_llm_config.get('temperature', 0.6))
                        max_tokens = agent_config.get('max_tokens', second_llm_config.get('max_tokens', 2500))
                    elif agent_config.get('model'):
                        model_name = agent_config.get('model')
                        temperature = agent_config.get('temperature', 0.6)
                        max_tokens = agent_config.get('max_tokens', 2500)
                    else:
                        # Fall back to system settings
                        model_name = system.llm_settings.get("model", "qwen3:30b-a3b")
                        temperature = 0.6
                        max_tokens = 2500
                else:
                    # No synthesizer agent found, use system settings
                    model_name = system.llm_settings.get("model", "qwen3:30b-a3b")
                    temperature = 0.6
                    max_tokens = 2500
                
                synthesizer_config = LLMConfig(
                    model_name=model_name,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                synthesizer_llm = OllamaLLM(synthesizer_config, base_url=system.ollama_base_url)
                
                synthesis_response = ""
                synthesis_thinking = ""
                in_synthesis_thinking = False
                synthesis_thinking_detected = False
                
                async for response_chunk in synthesizer_llm.generate_stream(synthesis_prompt):
                    chunk_text = response_chunk.text
                    synthesis_response += chunk_text
                    
                    if "<think>" in chunk_text:
                        in_synthesis_thinking = True
                        synthesis_thinking_detected = True
                        
                        yield json.dumps({
                            "type": "synthesis_thinking_start",
                            "agent": "synthesizer",
                            "message": "üí≠ Synthesizing insights..."
                        }) + "\n"
                        
                        think_start = chunk_text.find("<think>") + 7
                        synthesis_thinking = chunk_text[think_start:]
                        
                    elif "</think>" in chunk_text:
                        in_synthesis_thinking = False
                        
                        think_end = chunk_text.find("</think>")
                        synthesis_thinking += chunk_text[:think_end]
                        
                        yield json.dumps({
                            "type": "synthesis_thinking_complete",
                            "agent": "synthesizer",
                            "thinking": synthesis_thinking.strip()
                        }) + "\n"
                        
                        remaining_content = chunk_text[think_end + 8:]
                        if remaining_content.strip():
                            yield json.dumps({
                                "type": "agent_token",
                                "agent": "synthesizer",
                                "token": remaining_content
                            }) + "\n"
                            
                    elif in_synthesis_thinking:
                        synthesis_thinking += chunk_text
                        
                    else:
                        if chunk_text.strip():
                            yield json.dumps({
                                "type": "agent_token",
                                "agent": "synthesizer",
                                "token": chunk_text
                            }) + "\n"
                    
                    await asyncio.sleep(0.02)
                
                final_answer = system._clean_llm_response(synthesis_response)
                
            except Exception as e:
                logger.error(f"Synthesis failed: {e}")
                final_answer = f"Collaborative analysis from {len(agents_used)} specialized agents provides comprehensive insights into {question}."
            
            # Final response
            yield json.dumps({
                "type": "final_response",
                "response": final_answer,
                "agents_used": agents_used,
                "collaboration_metrics": {
                    "agent_communications": len(agent_communications),
                    "agents_with_tools": len([a for a in agents_used if available_agents[a].get('tools')]),
                    "thinking_sessions_detected": sum(1 for agent in agents_used if agent in agent_responses) + (1 if synthesis_thinking_detected else 0)
                },
                "execution_pattern": "natural_thinking_collaborative",
                "confidence_score": 0.92,
                "conversation_id": conversation_id,
                "message": "üéâ Natural thinking multi-agent analysis completed"
            }) + "\n"
            
        except Exception as e:
            logger.error(f"Natural thinking multi-agent streaming failed: {e}")
            yield json.dumps({
                "type": "error",
                "error": str(e),
                "message": "‚ùå Multi-agent execution failed"
            }) + "\n"
    
    return stream_natural_thinking()