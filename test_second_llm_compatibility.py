#!/usr/bin/env python3
"""
Test second_llm compatibility with both Qwen3 models.
This script validates that second_llm works correctly with both model variants
across different system components (task decomposer, multi-agent, etc.).
"""

import asyncio
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.agents.task_decomposer import TaskDecomposer
from app.core.llm_settings_cache import get_llm_settings, get_second_llm_full_config_with_detection
from app.llm.response_analyzer import clear_behavior_cache

async def test_task_decomposer_with_model(model_name: str):
    """Test task decomposer with a specific second_llm model"""
    
    print(f"\n{'='*80}")
    print(f"TESTING TASK DECOMPOSER WITH: {model_name}")
    print("="*80)
    
    # Update second_llm model in settings
    settings = get_llm_settings()
    original_model = settings.get('second_llm', {}).get('model', '')
    
    if 'second_llm' not in settings:
        settings['second_llm'] = {}
    settings['second_llm']['model'] = model_name
    
    print(f"Second LLM model set to: {model_name}")
    
    try:
        # Create task decomposer
        decomposer = TaskDecomposer()
        
        # Test with a complex task
        test_task = "Create a comprehensive marketing strategy for a new AI-powered banking app, including market research, competitor analysis, target audience identification, and launch plan."
        
        print(f"Test task: {test_task}")
        print("Testing task decomposition...")
        
        # Test the LLM call method directly
        prompt = f"Break down this complex task into manageable subtasks:\n\n{test_task}\n\nProvide a clear, numbered list of specific subtasks."
        
        response = await decomposer._call_llm(prompt, temperature=0.7, max_tokens=1000)
        
        print(f"\nResponse received:")
        print(f"- Length: {len(response)} characters")
        print(f"- First 200 chars: {response[:200]}...")
        
        # Check if response contains thinking tags (should be removed)
        has_thinking_tags = '<think>' in response.lower()
        print(f"- Contains thinking tags: {has_thinking_tags}")
        
        if has_thinking_tags:
            print("‚ùå ISSUE: Response still contains thinking tags")
            return False
        else:
            print("‚úÖ SUCCESS: Response properly processed")
            return True
            
    except Exception as e:
        print(f"‚ùå ERROR: Task decomposer test failed: {e}")
        return False
    finally:
        # Restore original model
        if original_model:
            settings['second_llm']['model'] = original_model

async def test_dynamic_detection_direct():
    """Test the dynamic detection functions directly"""
    
    print(f"\n{'='*80}")
    print("TESTING DYNAMIC DETECTION FUNCTIONS")
    print("="*80)
    
    models_to_test = [
        "qwen3:30b-a3b-q4_K_M",
        "qwen3:30b-a3b-instruct-2507-q4_K_M"
    ]
    
    # Sample responses that simulate what each model would generate
    sample_responses = {
        "qwen3:30b-a3b-q4_K_M": "<think>\nI need to break this down into steps...\n</think>\n\nHere are the main subtasks:\n1. Market research\n2. Competitor analysis",
        "qwen3:30b-a3b-instruct-2507-q4_K_M": "Based on your requirements, here are the main subtasks for developing a comprehensive marketing strategy:\n\n1. Market research and analysis\n2. Competitor landscape evaluation\n3. Target audience identification\n4. Product positioning strategy"
    }
    
    for model in models_to_test:
        print(f"\nüîç Testing: {model}")
        
        # Test configuration with detection
        sample_response = sample_responses.get(model, "")
        config = get_second_llm_full_config_with_detection(sample_response=sample_response)
        
        print(f"- Configured mode: {config.get('configured_mode', 'unknown')}")
        print(f"- Effective mode: {config.get('effective_mode', 'unknown')}")
        print(f"- Mode overridden: {config.get('mode_overridden', False)}")
        
        # Test behavior detection
        from app.llm.response_analyzer import detect_model_thinking_behavior
        is_thinking, confidence = detect_model_thinking_behavior(sample_response, model)
        
        print(f"- Detected thinking: {is_thinking}")
        print(f"- Detection confidence: {confidence:.2f}")
        
        expected_thinking = "instruct-2507" not in model
        if is_thinking == expected_thinking:
            print("‚úÖ Detection matches expectation")
        else:
            print("‚ùå Detection mismatch")

async def test_behavior_caching():
    """Test that behavior profiles are being cached correctly"""
    
    print(f"\n{'='*80}")
    print("TESTING BEHAVIOR CACHING")
    print("="*80)
    
    from app.llm.response_analyzer import response_analyzer
    
    # Get cache statistics
    stats = response_analyzer.get_cache_stats()
    
    print(f"Cache statistics:")
    print(f"- Total models cached: {stats['total_models']}")
    print(f"- Thinking models: {stats['thinking_models']}")
    print(f"- Non-thinking models: {stats['non_thinking_models']}")
    
    if stats['models']:
        print(f"\nCached model profiles:")
        for model, profile in stats['models'].items():
            behavior = "thinking" if profile['is_thinking'] else "non-thinking"
            print(f"  ‚Ä¢ {model}: {behavior} (confidence: {profile['confidence']:.2f}, samples: {profile['response_count']})")
    
    return stats['total_models'] > 0

async def main():
    """Run comprehensive second_llm compatibility tests"""
    
    print("Second LLM Compatibility Test Suite")
    print("="*80)
    
    # Clear behavior cache for clean test
    clear_behavior_cache()
    
    # Test 1: Dynamic detection functions
    await test_dynamic_detection_direct()
    
    # Test 2: Task decomposer with both models
    models_to_test = [
        "qwen3:30b-a3b-q4_K_M",
        "qwen3:30b-a3b-instruct-2507-q4_K_M"
    ]
    
    test_results = {}
    
    for model in models_to_test:
        try:
            success = await test_task_decomposer_with_model(model)
            test_results[model] = success
        except Exception as e:
            print(f"‚ùå Failed to test {model}: {e}")
            test_results[model] = False
        
        # Brief pause between tests
        await asyncio.sleep(1)
    
    # Test 3: Behavior caching
    cache_working = await test_behavior_caching()
    
    # Final summary
    print(f"\n{'='*80}")
    print("SECOND LLM COMPATIBILITY SUMMARY")
    print("="*80)
    
    all_passed = True
    
    for model, success in test_results.items():
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{status} {model}")
        if not success:
            all_passed = False
    
    cache_status = "‚úÖ WORKING" if cache_working else "‚ùå NOT WORKING"
    print(f"{cache_status} Behavior caching system")
    
    if not cache_working:
        all_passed = False
    
    print(f"\nüéØ OVERALL RESULT")
    print("-" * 60)
    
    if all_passed:
        print("üéâ SUCCESS: All second_llm compatibility tests passed!")
        print("‚úÖ Both Qwen3 models work with second_llm components")
        print("‚úÖ Dynamic detection is functioning correctly")
        print("‚úÖ Response processing handles both model types")
        print("‚úÖ Behavior caching is operational")
    else:
        print("‚ùå FAILURE: Some second_llm compatibility tests failed")
        print("‚ö†Ô∏è  Check the specific failures above for details")
    
    print(f"\nüìã SYSTEM STATUS")
    print("-" * 60)
    print("‚úÖ Task Decomposer: Enhanced with dynamic detection")
    print("‚úÖ Multi-Agent Streaming: Updated with response processing")
    print("‚úÖ Continuity Manager: Added thinking tag removal")
    print("‚úÖ Configuration System: Supports dynamic mode override")
    
    print(f"\nüöÄ NEXT STEPS")
    print("-" * 60)
    print("1. If tests pass: Second LLM is ready for both model variants")
    print("2. If tests fail: Check model availability and configuration")
    print("3. Test with real multi-agent workflows to validate end-to-end")
    print("4. Monitor performance with different models in production")

if __name__ == "__main__":
    asyncio.run(main())